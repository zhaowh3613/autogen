"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[4124],{24368:(e,s,n)=>{n.r(s),n.d(s,{assets:()=>c,contentTitle:()=>a,default:()=>m,frontMatter:()=>r,metadata:()=>o,toc:()=>l});var t=n(85893),i=n(11151);const r={sidebar_label:"transforms",title:"agentchat.contrib.capabilities.transforms"},a=void 0,o={id:"reference/agentchat/contrib/capabilities/transforms",title:"agentchat.contrib.capabilities.transforms",description:"MessageTransform",source:"@site/docs/reference/agentchat/contrib/capabilities/transforms.md",sourceDirName:"reference/agentchat/contrib/capabilities",slug:"/reference/agentchat/contrib/capabilities/transforms",permalink:"/autogen/docs/reference/agentchat/contrib/capabilities/transforms",draft:!1,unlisted:!1,editUrl:"https://github.com/microsoft/autogen/edit/main/website/docs/reference/agentchat/contrib/capabilities/transforms.md",tags:[],version:"current",frontMatter:{sidebar_label:"transforms",title:"agentchat.contrib.capabilities.transforms"},sidebar:"referenceSideBar",previous:{title:"transform_messages",permalink:"/autogen/docs/reference/agentchat/contrib/capabilities/transform_messages"},next:{title:"vision_capability",permalink:"/autogen/docs/reference/agentchat/contrib/capabilities/vision_capability"}},c={},l=[{value:"MessageTransform",id:"messagetransform",level:2},{value:"apply_transform",id:"apply_transform",level:3},{value:"MessageHistoryLimiter",id:"messagehistorylimiter",level:2},{value:"__init__",id:"__init__",level:3},{value:"apply_transform",id:"apply_transform-1",level:3},{value:"MessageTokenLimiter",id:"messagetokenlimiter",level:2},{value:"__init__",id:"__init__-1",level:3},{value:"apply_transform",id:"apply_transform-2",level:3}];function d(e){const s={code:"code",em:"em",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.a)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(s.h2,{id:"messagetransform",children:"MessageTransform"}),"\n",(0,t.jsx)(s.pre,{children:(0,t.jsx)(s.code,{className:"language-python",children:"class MessageTransform(Protocol)\n"})}),"\n",(0,t.jsx)(s.p,{children:"Defines a contract for message transformation."}),"\n",(0,t.jsxs)(s.p,{children:["Classes implementing this protocol should provide an ",(0,t.jsx)(s.code,{children:"apply_transform"})," method\nthat takes a list of messages and returns the transformed list."]}),"\n",(0,t.jsx)(s.h3,{id:"apply_transform",children:"apply_transform"}),"\n",(0,t.jsx)(s.pre,{children:(0,t.jsx)(s.code,{className:"language-python",children:"def apply_transform(messages: List[Dict]) -> List[Dict]\n"})}),"\n",(0,t.jsx)(s.p,{children:"Applies a transformation to a list of messages."}),"\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"Arguments"}),":"]}),"\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.code,{children:"messages"})," - A list of dictionaries representing messages."]}),"\n"]}),"\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"Returns"}),":"]}),"\n",(0,t.jsx)(s.p,{children:"A new list of dictionaries containing the transformed messages."}),"\n",(0,t.jsx)(s.h2,{id:"messagehistorylimiter",children:"MessageHistoryLimiter"}),"\n",(0,t.jsx)(s.pre,{children:(0,t.jsx)(s.code,{className:"language-python",children:"class MessageHistoryLimiter()\n"})}),"\n",(0,t.jsx)(s.p,{children:"Limits the number of messages considered by an agent for response generation."}),"\n",(0,t.jsx)(s.p,{children:"This transform keeps only the most recent messages up to the specified maximum number of messages (max_messages).\nIt trims the conversation history by removing older messages, retaining only the most recent messages."}),"\n",(0,t.jsx)(s.h3,{id:"__init__",children:"__init__"}),"\n",(0,t.jsx)(s.pre,{children:(0,t.jsx)(s.code,{className:"language-python",children:"def __init__(max_messages: Optional[int] = None)\n"})}),"\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"Arguments"}),":"]}),"\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.code,{children:"max_messages"})," ",(0,t.jsx)(s.em,{children:"None or int"})," - Maximum number of messages to keep in the context.\nMust be greater than 0 if not None."]}),"\n"]}),"\n",(0,t.jsx)(s.h3,{id:"apply_transform-1",children:"apply_transform"}),"\n",(0,t.jsx)(s.pre,{children:(0,t.jsx)(s.code,{className:"language-python",children:"def apply_transform(messages: List[Dict]) -> List[Dict]\n"})}),"\n",(0,t.jsx)(s.p,{children:"Truncates the conversation history to the specified maximum number of messages."}),"\n",(0,t.jsx)(s.p,{children:"This method returns a new list containing the most recent messages up to the specified\nmaximum number of messages (max_messages). If max_messages is None, it returns the\noriginal list of messages unmodified."}),"\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"Arguments"}),":"]}),"\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.code,{children:"messages"})," ",(0,t.jsx)(s.em,{children:"List[Dict]"})," - The list of messages representing the conversation history."]}),"\n"]}),"\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"Returns"}),":"]}),"\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.code,{children:"List[Dict]"})," - A new list containing the most recent messages up to the specified maximum."]}),"\n"]}),"\n",(0,t.jsx)(s.h2,{id:"messagetokenlimiter",children:"MessageTokenLimiter"}),"\n",(0,t.jsx)(s.pre,{children:(0,t.jsx)(s.code,{className:"language-python",children:"class MessageTokenLimiter()\n"})}),"\n",(0,t.jsx)(s.p,{children:"Truncates messages to meet token limits for efficient processing and response generation."}),"\n",(0,t.jsx)(s.p,{children:"This transformation applies two levels of truncation to the conversation history:"}),"\n",(0,t.jsxs)(s.ol,{children:["\n",(0,t.jsx)(s.li,{children:"Truncates each individual message to the maximum number of tokens specified by max_tokens_per_message."}),"\n",(0,t.jsx)(s.li,{children:"Truncates the overall conversation history to the maximum number of tokens specified by max_tokens."}),"\n"]}),"\n",(0,t.jsx)(s.p,{children:"NOTE: Tokens are counted using the encoder for the specified model. Different models may yield different token\ncounts for the same text."}),"\n",(0,t.jsx)(s.p,{children:"NOTE: For multimodal LLMs, the token count may be inaccurate as it does not account for the non-text input\n(e.g images)."}),"\n",(0,t.jsx)(s.p,{children:"The truncation process follows these steps in order:"}),"\n",(0,t.jsxs)(s.ol,{children:["\n",(0,t.jsx)(s.li,{children:"Messages are processed in reverse order (newest to oldest)."}),"\n",(0,t.jsx)(s.li,{children:"Individual messages are truncated based on max_tokens_per_message. For multimodal messages containing both text\nand other types of content, only the text content is truncated."}),"\n",(0,t.jsx)(s.li,{children:"The overall conversation history is truncated based on the max_tokens limit. Once the accumulated token count\nexceeds this limit, the current message being processed as well as any remaining messages are discarded."}),"\n",(0,t.jsx)(s.li,{children:"The truncated conversation history is reconstructed by prepending the messages to a new list to preserve the\noriginal message order."}),"\n"]}),"\n",(0,t.jsx)(s.h3,{id:"__init__-1",children:"__init__"}),"\n",(0,t.jsx)(s.pre,{children:(0,t.jsx)(s.code,{className:"language-python",children:'def __init__(max_tokens_per_message: Optional[int] = None,\n             max_tokens: Optional[int] = None,\n             model: str = "gpt-3.5-turbo-0613")\n'})}),"\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"Arguments"}),":"]}),"\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.code,{children:"max_tokens_per_message"})," ",(0,t.jsx)(s.em,{children:"None or int"})," - Maximum number of tokens to keep in each message.\nMust be greater than or equal to 0 if not None."]}),"\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.code,{children:"max_tokens"})," ",(0,t.jsx)(s.em,{children:"Optional[int]"})," - Maximum number of tokens to keep in the chat history.\nMust be greater than or equal to 0 if not None."]}),"\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.code,{children:"model"})," ",(0,t.jsx)(s.em,{children:"str"})," - The target OpenAI model for tokenization alignment."]}),"\n"]}),"\n",(0,t.jsx)(s.h3,{id:"apply_transform-2",children:"apply_transform"}),"\n",(0,t.jsx)(s.pre,{children:(0,t.jsx)(s.code,{className:"language-python",children:"def apply_transform(messages: List[Dict]) -> List[Dict]\n"})}),"\n",(0,t.jsx)(s.p,{children:"Applies token truncation to the conversation history."}),"\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"Arguments"}),":"]}),"\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.code,{children:"messages"})," ",(0,t.jsx)(s.em,{children:"List[Dict]"})," - The list of messages representing the conversation history."]}),"\n"]}),"\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"Returns"}),":"]}),"\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.code,{children:"List[Dict]"})," - A new list containing the truncated messages up to the specified token limits."]}),"\n"]})]})}function m(e={}){const{wrapper:s}={...(0,i.a)(),...e.components};return s?(0,t.jsx)(s,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},11151:(e,s,n)=>{n.d(s,{Z:()=>o,a:()=>a});var t=n(67294);const i={},r=t.createContext(i);function a(e){const s=t.useContext(r);return t.useMemo((function(){return"function"==typeof e?e(s):{...s,...e}}),[s,e])}function o(e){let s;return s=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),t.createElement(r.Provider,{value:s},e.children)}}}]);